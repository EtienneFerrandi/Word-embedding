{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Word embedding CBW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMBTeYGDgnIq+n9xzc4eX5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EtienneFerrandi/Word-embedding/blob/main/Word_embedding_CBW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pntgvg_aYwd8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import string"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zKL1-frblGQ"
      },
      "source": [
        "with open(\"AU_s_9.txt\", \"r\") as raw_text:\n",
        "    raw_text = raw_text.read().split()\n",
        "raw_text=[''.join(c for c in s if c not in string.punctuation) for s in raw_text]  #on enlève la ponctuation de la liste de vocabulaire\n",
        "raw_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaYvhloqTWOw"
      },
      "source": [
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwAgsGDNTWSN"
      },
      "source": [
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYsAJgTTY0lB",
        "outputId": "56a7c7e7-0c83-4f8b-fd45-5a958f9409e0"
      },
      "source": [
        "CONTEXT_SIZE = 2  #on créé un environnement de deux mots à droite, deux mots à gauche, comme il convient pour une approche CBW\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['Tractatus', 'sancti', 'de', 'decem'], 'Augustini'), (['sancti', 'Augustini', 'decem', 'chordis'], 'de'), (['Augustini', 'de', 'chordis', 'sermo'], 'decem'), (['de', 'decem', 'sermo', 'habitus'], 'chordis'), (['decem', 'chordis', 'habitus', 'Chusa'], 'sermo')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsjDUHB4WBGf"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size) #définition de l'hyperparamètre d'apprentissage\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        embeds = torch.sum(embeds, dim=0).view(1,-1)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs #définition de la propagation avant et calcul de la couche de sortie"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sweGDB-iXeYK"
      },
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJakyJpFWjh0",
        "outputId": "e3149b92-43d6-409b-d0ff-35c27ea685b8"
      },
      "source": [
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = CBOW(vocab_size, embedding_dim=20)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001) #calcul du descente de gradient stochastique\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        context_idxs = make_context_vector(context, word_to_ix)\n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_idxs)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)  #calcul de la fonction de perte\n",
        "\n",
        "print(losses) "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[53293.108147382736, 50253.469131708145, 48751.78603184223, 47672.16461801529, 46768.78481078148, 45960.62044394016, 45203.96868211031, 44470.8713760376, 43743.13012647629, 43007.847942113876, 42257.127690792084, 41483.61944037676, 40683.2115137279, 39853.445836126804, 38991.53594198823, 38096.115712314844, 37167.10522556305, 36204.85303398967, 35211.43820346892, 34189.33372756094, 33141.734033979475, 32072.105412974954, 30986.086135480553, 29888.992124021053, 28787.422549234703, 27688.314651513472, 26598.935599895194, 25526.199487818405, 24477.362013080157, 23459.277985659428, 22477.484794866294, 21536.75844200235, 20640.294699640945, 19790.20813646447, 18987.552151189186, 18231.745167727582, 17521.761984748766, 16855.904907517135, 16231.31908501871, 15645.655530881137, 15095.758815924637, 14579.044319703244, 14093.003472141456, 13635.125820330344, 13202.936497669434, 12794.77383004874, 12408.665450908476, 12042.804421952693, 11695.461307525402, 11366.054533780552, 11053.057803976117, 10754.827062772238, 10471.011810099124, 10200.255393888336, 9941.904623564798, 9694.758264884003, 9458.233397750882, 9232.451796304027, 9015.810507293907, 8808.293772949488, 8609.061208945932, 8417.55430383375, 8232.598283755302, 8054.832236229209, 7883.474332443089, 7717.969600198441, 7558.1106457160495, 7403.748970433138, 7254.11458069511, 7108.980926188757, 6968.617811150936, 6832.233685483341, 6699.459774301882, 6570.4889986130875, 6445.213407148229, 6323.314377445466, 6204.464908699796, 6088.873286088841, 5975.9560264461325, 5866.682965329252, 5759.050764746251, 5654.424198708468, 5552.645839055171, 5452.611908315666, 5354.936650761141, 5259.813213886213, 5166.173496503674, 5074.539266349166, 4985.159007533162, 4897.202594220595, 4811.416147236334, 4727.068831993733, 4644.292716734431, 4563.232619715331, 4483.460826839051, 4405.437346772371, 4328.76255604098, 4253.746484574294, 4179.979856209677, 4107.622270842221]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1if6z3ca92u",
        "outputId": "d145ea03-66b6-4583-c5a6-d7a1f74ac85a"
      },
      "source": [
        "# Test\n",
        "with torch.no_grad():\n",
        "    context = ['tunc', 'dies', 'diceretur', 'hodiernus'] # le mot cible est 'qui'\n",
        "    context_vector = make_context_vector(context, word_to_ix)\n",
        "    predict = model(context_vector)\n",
        "    print(ix_to_word[predict.argmax(dim=1).item()])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qui\n"
          ]
        }
      ]
    }
  ]
}